{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce746e3f",
   "metadata": {},
   "source": [
    "# 实验二（2025 夏季）——前馈神经网络：回归/二分类/多分类（手动实现与torch.nn）\n",
    "\n",
    "- 按课程任务与图片要求，实现如下内容：\n",
    "  - 手动实现前馈网络解决回归、二分类、多分类（MNIST），并绘制训练/测试 loss 曲线\n",
    "  - 使用 torch.nn 实现相同任务，并绘制训练/测试 loss 曲线\n",
    "  - 多分类任务上对比至少三种激活函数（ReLU/Tanh/LeakyReLU），并研究隐藏层层数与隐藏单元数的影响\n",
    "  - 在多分类任务中，分别“手动实现”和“torch.nn 实现” dropout 与 L2 正则，并分析不同超参的影响\n",
    "  - 回归/二分类/多分类分别选取最优模型，做 10 折交叉验证，表格展示每折结果\n",
    "- 按图片要求，合成数据：\n",
    "  - 回归：N=10000（train=7000/test=3000），p=500，高维线性：y=0.028+\\sum 0.0056 x_i + ε\n",
    "  - 二分类：共两个数据集，N=10000（train=7000/test=3000），p=200，两个类特征来自均值互为相反数、方差相同的正态分布\n",
    "  - 多分类：使用 MNIST（28x28 展平为 784）\n",
    "\n",
    "说明：代码默认在 CPU 可运行，epoch/网格规模已控制到课程实验可完成的量，如需更高精度可增大 epoch。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5756d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 公共依赖与工具\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "def set_seed(s=42):\n",
    "    np.random.seed(s); torch.manual_seed(s)\n",
    "\n",
    "def split_train_test(X, y, n_train):\n",
    "    Xtr, Xte = X[:n_train], X[n_train:]\n",
    "    ytr, yte = y[:n_train], y[n_train:]\n",
    "    return Xtr, ytr, Xte, yte\n",
    "\n",
    "def plot_curves(history, title):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1); plt.plot(history['train_loss'], label='train');\n",
    "    if 'test_loss' in history: plt.plot(history['test_loss'], label='test');\n",
    "    plt.title(title+' Loss'); plt.legend()\n",
    "    if 'train_acc' in history:\n",
    "        plt.subplot(1,2,2); plt.plot(history['train_acc'], label='train');\n",
    "        if 'test_acc' in history: plt.plot(history['test_acc'], label='test');\n",
    "        plt.title(title+' Acc'); plt.legend()\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务A：回归（手动前馈 + torch.nn）——N=10000, p=500\n",
    "set_seed(0)\n",
    "N, P = 10000, 500\n",
    "X = np.random.randn(N, P)\n",
    "true_w = np.full((P,), 0.0056)\n",
    "y = 0.028 + X @ true_w + 0.1*np.random.randn(N)\n",
    "Xtr, ytr, Xte, yte = split_train_test(X, y, 7000)\n",
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr_t = torch.tensor(ytr, dtype=torch.float32).view(-1,1)\n",
    "Xte_t = torch.tensor(Xte, dtype=torch.float32)\n",
    "yte_t = torch.tensor(yte, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "# 手动前馈（1隐层）\n",
    "class MLPRegScratch:\n",
    "    def __init__(self, p, h):\n",
    "        self.W1 = torch.randn(p, h, requires_grad=True)*0.01\n",
    "        self.b1 = torch.zeros(h, requires_grad=True)\n",
    "        self.W2 = torch.randn(h, 1, requires_grad=True)*0.01\n",
    "        self.b2 = torch.zeros(1, requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        z1 = x @ self.W1 + self.b1\n",
    "        a1 = torch.relu(z1)\n",
    "        out = a1 @ self.W2 + self.b2\n",
    "        return out\n",
    "    def params(self):\n",
    "        return [self.W1,self.b1,self.W2,self.b2]\n",
    "\n",
    "def train_reg_scratch(model, Xtr, ytr, Xte, yte, lr=1e-2, epochs=50, bs=256):\n",
    "    ds = TensorDataset(Xtr, ytr); dl = DataLoader(ds, batch_size=bs, shuffle=True)\n",
    "    hist={'train_loss':[], 'test_loss':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl=0; n=0\n",
    "        for xb,yb in dl:\n",
    "            pred = model.forward(xb)\n",
    "            loss = torch.mean((pred-yb)**2)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.params(): p -= lr*p.grad; p.grad.zero_()\n",
    "            tl += loss.item()*len(xb); n+=len(xb)\n",
    "        with torch.no_grad():\n",
    "            te = torch.mean((model.forward(Xte)-yte)**2).item()\n",
    "        hist['train_loss'].append(tl/n); hist['test_loss'].append(te)\n",
    "        if ep%10==0: print(f'[Reg-S] Ep{ep:3d} | trainMSE={tl/n:.4f} | testMSE={te:.4f}')\n",
    "    return hist\n",
    "\n",
    "scratch = MLPRegScratch(P, 128)\n",
    "h1 = train_reg_scratch(scratch, Xtr_t,ytr_t,Xte_t,yte_t, lr=5e-3, epochs=60)\n",
    "plot_curves(h1,'Reg Scratch')\n",
    "\n",
    "# torch.nn 版\n",
    "class MLPReg(nn.Module):\n",
    "    def __init__(self,p,h):\n",
    "        super().__init__(); self.net=nn.Sequential(\n",
    "            nn.Linear(p,h), nn.ReLU(), nn.Linear(h,1))\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "def train_reg_torch(model, Xtr,ytr,Xte,yte, lr=1e-2, epochs=60, bs=256):\n",
    "    ds=TensorDataset(Xtr,ytr); dl=DataLoader(ds,batch_size=bs,shuffle=True)\n",
    "    opt=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    hist={'train_loss':[], 'test_loss':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl=0;n=0\n",
    "        for xb,yb in dl:\n",
    "            pred=model(xb); loss=F.mse_loss(pred,yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tl += loss.item()*len(xb); n+=len(xb)\n",
    "        with torch.no_grad():\n",
    "            te=F.mse_loss(model(Xte), yte).item()\n",
    "        hist['train_loss'].append(tl/n); hist['test_loss'].append(te)\n",
    "        if ep%10==0: print(f'[Reg-N] Ep{ep:3d} | trainMSE={tl/n:.4f} | testMSE={te:.4f}')\n",
    "    return hist\n",
    "\n",
    "m = MLPReg(P,128)\n",
    "h2 = train_reg_torch(m,Xtr_t,ytr_t,Xte_t,yte_t, lr=1e-3, epochs=60)\n",
    "plot_curves(h2,'Reg Torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ae641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务B：二分类（两个数据集、p=200，均值相反数、方差相同），手动+torch.nn\n",
    "set_seed(1)\n",
    "N, P = 10000, 200\n",
    "mu = 2.0; sigma = 1.0\n",
    "X0 = np.random.randn(N//2, P)*sigma + mu\n",
    "X1 = np.random.randn(N//2, P)*sigma - mu\n",
    "X  = np.vstack([X0,X1]); y = np.hstack([np.zeros(N//2), np.ones(N//2)])\n",
    "idx=np.arange(N); np.random.shuffle(idx); X=X[idx]; y=y[idx]\n",
    "Xtr,ytr,Xte,yte = split_train_test(X,y,7000)\n",
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr_t = torch.tensor(ytr, dtype=torch.float32).view(-1,1)\n",
    "Xte_t = torch.tensor(Xte, dtype=torch.float32)\n",
    "yte_t = torch.tensor(yte, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "# 手动前馈（分类）\n",
    "class MLPBinScratch:\n",
    "    def __init__(self,p,h):\n",
    "        self.W1=torch.randn(p,h,requires_grad=True)*0.01\n",
    "        self.b1=torch.zeros(h,requires_grad=True)\n",
    "        self.W2=torch.randn(h,1,requires_grad=True)*0.01\n",
    "        self.b2=torch.zeros(1,requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        a=torch.relu(x@self.W1+self.b1)\n",
    "        z=a@self.W2+self.b2\n",
    "        return torch.sigmoid(z)\n",
    "    def params(self): return [self.W1,self.b1,self.W2,self.b2]\n",
    "\n",
    "def train_bin_scratch(model,Xtr,ytr,Xte,yte,lr=5e-3,epochs=40,bs=128):\n",
    "    ds=TensorDataset(Xtr,ytr); dl=DataLoader(ds,batch_size=bs,shuffle=True)\n",
    "    hist={'train_loss':[],'test_loss':[],'train_acc':[],'test_acc':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl=0;ta=0;n=0\n",
    "        for xb,yb in dl:\n",
    "            p=model.forward(xb)\n",
    "            loss=-torch.mean(yb*torch.log(p+1e-9)+(1-yb)*torch.log(1-p+1e-9))\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for pm in model.params(): pm -= lr*pm.grad; pm.grad.zero_()\n",
    "            tl+=loss.item()*len(xb); ta+=torch.sum((p>0.5).float().eq(yb)).item(); n+=len(xb)\n",
    "        with torch.no_grad():\n",
    "            pt=model.forward(Xte)\n",
    "            te=-torch.mean(yte*torch.log(pt+1e-9)+(1-yte)*torch.log(1-pt+1e-9)).item()\n",
    "            ta_tr=ta/n; ta_te=torch.mean((pt>0.5).float().eq(yte)).item()\n",
    "        hist['train_loss'].append(tl/n); hist['test_loss'].append(te)\n",
    "        hist['train_acc'].append(ta_tr); hist['test_acc'].append(ta_te)\n",
    "        if ep%10==0: print(f'[Bin-S] Ep{ep:3d} | trLoss={tl/n:.4f} teLoss={te:.4f} | trAcc={ta_tr:.4f} teAcc={ta_te:.4f}')\n",
    "    return hist\n",
    "\n",
    "hS = train_bin_scratch(MLPBinScratch(P,256), Xtr_t,ytr_t,Xte_t,yte_t)\n",
    "plot_curves(hS,'Binary Scratch')\n",
    "\n",
    "# torch.nn\n",
    "class MLPBin(nn.Module):\n",
    "    def __init__(self,p,h):\n",
    "        super().__init__(); self.net=nn.Sequential(\n",
    "            nn.Linear(p,h), nn.ReLU(), nn.Linear(h,1))\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "def train_bin_torch(m,Xtr,ytr,Xte,yte,lr=1e-3,epochs=40,bs=128):\n",
    "    ds=TensorDataset(Xtr,ytr); dl=DataLoader(ds,batch_size=bs,shuffle=True)\n",
    "    crit=nn.BCEWithLogitsLoss(); opt=torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    hist={'train_loss':[],'test_loss':[],'train_acc':[],'test_acc':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl=0;ta=0;n=0\n",
    "        for xb,yb in dl:\n",
    "            z=m(xb); loss=crit(z,yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tl+=loss.item()*len(xb); ta+=torch.sum((torch.sigmoid(z)>0.5).float().eq(yb)).item(); n+=len(xb)\n",
    "        with torch.no_grad():\n",
    "            zt=m(Xte); te=crit(zt,yte).item();\n",
    "            ta_tr=ta/n; ta_te=torch.mean((torch.sigmoid(zt)>0.5).float().eq(yte)).item()\n",
    "        hist['train_loss'].append(tl/n); hist['test_loss'].append(te)\n",
    "        hist['train_acc'].append(ta_tr); hist['test_acc'].append(ta_te)\n",
    "        if ep%10==0: print(f'[Bin-N] Ep{ep:3d} | trLoss={tl/n:.4f} teLoss={te:.4f} | trAcc={ta_tr:.4f} teAcc={ta_te:.4f}')\n",
    "    return hist\n",
    "\n",
    "hN = train_bin_torch(MLPBin(P,256), Xtr_t,ytr_t,Xte_t,yte_t)\n",
    "plot_curves(hN,'Binary Torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad19aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务C：MNIST 多分类 —— 激活函数 / 隐藏层配置 / Dropout / L2\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# 数据\n",
    "transform = T.Compose([T.ToTensor(), T.Lambda(lambda x: x.view(-1))])\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 取子集加速演示，可调大\n",
    "ntr=20000; nte=5000\n",
    "train_subset, _ = random_split(mnist_train, [ntr, len(mnist_train)-ntr])\n",
    "Xtr = torch.stack([d[0] for d in train_subset])\n",
    "Ytr = torch.tensor([d[1] for d in train_subset])\n",
    "Xte = torch.stack([mnist_test[i][0] for i in range(nte)])\n",
    "Yte = torch.tensor([mnist_test[i][1] for i in range(nte)])\n",
    "\n",
    "# 模型工厂\n",
    "class MLPCls(nn.Module):\n",
    "    def __init__(self, d=784, h=256, layers=1, act='relu', dropout=0.0):\n",
    "        super().__init__()\n",
    "        acts={'relu':nn.ReLU(),'tanh':nn.Tanh(),'leaky':nn.LeakyReLU(0.1)}\n",
    "        L=[]; inp=d\n",
    "        for i in range(layers):\n",
    "            L+=[nn.Linear(inp,h), acts[act]]\n",
    "            if dropout>0: L.append(nn.Dropout(dropout))\n",
    "            inp=h\n",
    "        L.append(nn.Linear(inp,10))\n",
    "        self.net=nn.Sequential(*L)\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "\n",
    "def train_eval(dconf, wd=0.0, epochs=10, bs=128):\n",
    "    ds = TensorDataset(Xtr, Ytr); dl=DataLoader(ds,batch_size=bs,shuffle=True)\n",
    "    m=MLPCls(**dconf)\n",
    "    opt=torch.optim.Adam(m.parameters(), lr=1e-3, weight_decay=wd)\n",
    "    crit=nn.CrossEntropyLoss()\n",
    "    hist={'train_loss':[],'test_loss':[],'train_acc':[],'test_acc':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl=0;ta=0;n=0\n",
    "        for xb,yb in dl:\n",
    "            z=m(xb); loss=crit(z,yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tl+=loss.item()*len(xb); ta+=torch.sum(z.argmax(1).eq(yb)).item(); n+=len(xb)\n",
    "        with torch.no_grad():\n",
    "            zt=m(Xte); te=crit(zt,Yte).item()\n",
    "            tr=ta/n; te_acc=zt.argmax(1).eq(Yte).float().mean().item()\n",
    "        hist['train_loss'].append(tl/n); hist['test_loss'].append(te)\n",
    "        hist['train_acc'].append(tr); hist['test_acc'].append(te_acc)\n",
    "        if ep%2==0: print(f\"[MNIST] ep={ep:2d} {dconf} wd={wd} | trAcc={tr:.3f} teAcc={te_acc:.3f}\")\n",
    "    return hist\n",
    "\n",
    "# 激活函数比较\n",
    "cfg_relu = {'d':784,'h':256,'layers':1,'act':'relu','dropout':0.0}\n",
    "cfg_tanh = {'d':784,'h':256,'layers':1,'act':'tanh','dropout':0.0}\n",
    "cfg_leak = {'d':784,'h':256,'layers':1,'act':'leaky','dropout':0.0}\n",
    "\n",
    "h_relu = train_eval(cfg_relu, wd=0.0)\n",
    "h_tanh = train_eval(cfg_tanh, wd=0.0)\n",
    "h_leak = train_eval(cfg_leak, wd=0.0)\n",
    "\n",
    "plot_curves(h_relu,'ReLU'); plot_curves(h_tanh,'Tanh'); plot_curves(h_leak,'LeakyReLU')\n",
    "\n",
    "# 隐藏层/单元数影响\n",
    "cfg_deep = {'d':784,'h':256,'layers':2,'act':'relu','dropout':0.0}\n",
    "h_deep = train_eval(cfg_deep, wd=0.0)\n",
    "plot_curves(h_deep,'2-Layers')\n",
    "\n",
    "# Dropout 与 L2 正则\n",
    "cfg_do = {'d':784,'h':256,'layers':2,'act':'relu','dropout':0.5}\n",
    "h_do = train_eval(cfg_do, wd=0.0)\n",
    "plot_curves(h_do,'Dropout-0.5')\n",
    "\n",
    "h_wd = train_eval(cfg_deep, wd=1e-4)\n",
    "plot_curves(h_wd,'L2-1e-4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务D：在多分类任务上，手动实现 Dropout 与 L2（简洁版）\n",
    "class LinearScratch:\n",
    "    def __init__(self, d, h):\n",
    "        self.W = torch.randn(d,h,requires_grad=True)*0.01\n",
    "        self.b = torch.zeros(h,requires_grad=True)\n",
    "    def __call__(self,x): return x@self.W + self.b\n",
    "    def params(self): return [self.W,self.b]\n",
    "\n",
    "class MLPDropScratch:\n",
    "    def __init__(self, d=784, h=256, pdrop=0.5):\n",
    "        self.l1 = LinearScratch(d,h)\n",
    "        self.l2 = LinearScratch(h,10)\n",
    "        self.pdrop = pdrop\n",
    "    def forward(self, x, train=True):\n",
    "        z1 = self.l1(x); a1 = torch.relu(z1)\n",
    "        if train and self.pdrop>0:\n",
    "            mask = (torch.rand_like(a1) > self.pdrop).float()\n",
    "            a1 = a1 * mask / (1.0-self.pdrop)\n",
    "        z2 = self.l2(a1)\n",
    "        return z2\n",
    "    def params(self): return self.l1.params()+self.l2.params()\n",
    "\n",
    "def train_scratch_cls(model, Xtr,Ytr,Xte,Yte, lr=1e-3, wd=0.0, epochs=8, bs=128):\n",
    "    ds=TensorDataset(Xtr,Ytr); dl=DataLoader(ds,batch_size=bs,shuffle=True)\n",
    "    hist={'train_loss':[],'test_loss':[],'train_acc':[],'test_acc':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl=0;ta=0;n=0\n",
    "        for xb,yb in dl:\n",
    "            logits = model.forward(xb, train=True)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            if wd>0:\n",
    "                l2=0\n",
    "                for p in model.params(): l2 += (p**2).sum()\n",
    "                loss = loss + wd*l2/len(xb)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.params(): p -= lr*p.grad; p.grad.zero_()\n",
    "            tl+=loss.item()*len(xb); ta+=torch.sum(logits.argmax(1).eq(yb)).item(); n+=len(xb)\n",
    "        with torch.no_grad():\n",
    "            zt = model.forward(Xte, train=False)\n",
    "            te = F.cross_entropy(zt, Yte).item()\n",
    "            tr=ta/n; te_acc=zt.argmax(1).eq(Yte).float().mean().item()\n",
    "        hist['train_loss'].append(tl/n); hist['test_loss'].append(te)\n",
    "        hist['train_acc'].append(tr); hist['test_acc'].append(te_acc)\n",
    "        print(f'[Scratch-DO/L2] ep={ep:2d} lr={lr} wd={wd} pdrop={model.pdrop} | trAcc={tr:.3f} teAcc={te_acc:.3f}')\n",
    "    return hist\n",
    "\n",
    "Xs, Ys = Xtr, Ytr  # 复用MNIST子集\n",
    "h_scr = train_scratch_cls(MLPDropScratch(784,256,0.5), Xs,Ys,Xte,Yte, lr=5e-4, wd=1e-4)\n",
    "plot_curves(h_scr,'Scratch Dropout+L2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14452d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务E：10折交叉验证（回归/二分类/多分类选最优配置各一）\n",
    "\n",
    "def kfold_eval(model_fn, loss_fn, X, y, k=10, task='reg'):\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y)\n",
    "    if task!='multi': y = y.view(-1,1).float()\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores=[]\n",
    "    for tr_idx, te_idx in kf.split(X):\n",
    "        m = model_fn()\n",
    "        if task=='reg':\n",
    "            opt=torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "            for _ in range(20):\n",
    "                loss=loss_fn(m(X[tr_idx]), y[tr_idx]); opt.zero_grad(); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                te=loss_fn(m(X[te_idx]), y[te_idx]).item(); scores.append(te)\n",
    "        elif task=='bin':\n",
    "            opt=torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "            for _ in range(20):\n",
    "                loss=loss_fn(m(X[tr_idx]), y[tr_idx]); opt.zero_grad(); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                z=m(X[te_idx]); acc=(torch.sigmoid(z)>0.5).float().eq(y[te_idx]).float().mean().item()\n",
    "                scores.append(acc)\n",
    "        else: # multi\n",
    "            opt=torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "            for _ in range(5):\n",
    "                loss=loss_fn(m(X[tr_idx]), y[tr_idx].long()); opt.zero_grad(); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc=m(X[te_idx]).argmax(1).eq(y[te_idx].long()).float().mean().item(); scores.append(acc)\n",
    "    return np.array(scores)\n",
    "\n",
    "# 回归（使用 MLPReg 最优配置示例）\n",
    "Xr, yr = X, y  # 来自任务A生成\n",
    "reg_scores = kfold_eval(lambda: MLPReg(P,128), F.mse_loss, Xr, yr, k=10, task='reg')\n",
    "print('Reg 10-fold MSE:', np.round(reg_scores,4), '| mean±std=', reg_scores.mean(), reg_scores.std())\n",
    "\n",
    "# 二分类（使用 MLPBin 最优配置示例）\n",
    "Xb, yb = X, y  # 任务B里最后一次生成可重新生成或复用\n",
    "Xb, yb = Xtr, ytr  # 简化：使用训练集做演示\n",
    "bin_scores = kfold_eval(lambda: MLPBin(P,256), lambda z,t: F.binary_cross_entropy_with_logits(z,t.float()), Xb, yb, k=10, task='bin')\n",
    "print('Bin 10-fold Acc:', np.round(bin_scores,4), '| mean±std=', bin_scores.mean(), bin_scores.std())\n",
    "\n",
    "# 多分类（使用 MLPCls 两层ReLU）\n",
    "Xm, ym = Xtr.numpy(), Ytr.numpy()\n",
    "multi_scores = kfold_eval(lambda: MLPCls(784,256,2,'relu',0.0), nn.CrossEntropyLoss(), Xm, ym, k=10, task='multi')\n",
    "print('Multi 10-fold Acc:', np.round(multi_scores,4), '| mean±std=', multi_scores.mean(), multi_scores.std())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
