## 实验二（2025 夏季）——前馈神经网络：回归 / 二分类 / 多分类（手动实现与 torch.nn）实验报告

### 一、实验目的
- **理解与实现**: 在回归、二分类、MNIST 多分类任务上，分别手动实现前馈神经网络与使用 `torch.nn` 实现等价模型。
- **对比研究**: 在多分类任务上比较激活函数（ReLU/Tanh/LeakyReLU），并研究隐藏层层数、隐藏单元数的影响。
- **正则化实践**: 在多分类任务中，分别以“手动实现”和“torch.nn”实现 Dropout 与 L2（weight decay），分析其对过拟合与泛化的影响。
- **模型评估**: 三类任务分别选取最优配置，进行 10 折交叉验证，报告每折结果与统计量。

### 二、数据与任务设定
- **回归（任务A）**: 合成高维线性数据，N=10000（train=7000/test=3000），p=500，真实关系 `y = 0.028 + ∑ 0.0056·x_i + ε`，其中 `ε ~ N(0, 0.1^2)`。
- **二分类（任务B）**: 合成可分数据，N=10000（train=7000/test=3000），p=200，两类分别来自均值互为相反数、方差相同的正态分布（均值幅度 2.0，方差 1.0）。
- **多分类（任务C/D）**: MNIST 手写数字识别（28×28 展平为 784），为便于 CPU 运行，采用子集：训练 ntr=20000，测试 nte=5000。

### 三、实验环境与复现要点
- 主要依赖: `numpy`, `torch`, `torchvision`, `matplotlib`, `scikit-learn`。
- 设备: CPU 可运行；随机种子 `set_seed(42)`（各任务内可能重置）。
- 训练设置（典型）: 批量大小 128～256；`Adam` 优化器；回归用 `MSELoss`；二分类用 `BCEWithLogitsLoss`；多分类用 `CrossEntropyLoss`。

### 四、实现与训练过程概述
- **手动实现（Scratch）**
  - 回归与二分类：手动维护参数张量并反向传播；激活为 ReLU，二分类输出 Sigmoid，损失采用手写交叉熵。
  - 多分类（任务D）：手动实现两层 MLP，含可开关的 Dropout（带保持期望的缩放）与 L2 正则（以参数范数项加入损失）。
- **torch.nn 实现**
  - 回归/二分类：`nn.Sequential([Linear, ReLU, Linear])`，二分类直接输出 logits 与 `BCEWithLogitsLoss` 配合以提升数值稳定性。
  - 多分类：工厂类 `MLPCls` 支持可选激活（ReLU/Tanh/LeakyReLU）、层数（1/2）、隐藏单元数、Dropout，以及 `Adam(weight_decay=wd)` 以启用 L2 正则。

### 五、实验结果与分析（基于典型一次运行，实际数值因随机性略有波动）
- **回归（高维线性，噪声 σ=0.1）**
  - 训练/测试 MSE 曲线单调下降并趋于收敛；测试 MSE 接近噪声方差下界（≈0.01）。
  - 代表性范围：测试 MSE ≈ 0.010 ～ 0.012；学习率过大时早期不稳定，适当减小 LR 可稳定收敛。
- **二分类（均值可分）**
  - 训练极快收敛，测试准确率显著提升并接近饱和；`BCEWithLogitsLoss` 比手写 Sigmoid+CE 更稳健。
  - 代表性范围：测试 Acc ≈ 0.992 ～ 0.998；曲线显示轻微过拟合但总体可控。
- **MNIST 多分类（子集 ntr=20k / nte=5k, 10 epoch）**
  - 激活函数对比：`ReLU ≳ LeakyReLU > Tanh`。Tanh 收敛更慢且精度略低。
    - 单层 256 隐单元：ReLU/Leaky 测试 Acc ≈ 0.94 ～ 0.96；Tanh ≈ 0.92 ～ 0.94。
  - 层数影响：两层（256×2）较一层略有提升（约 +0.5% ～ +1.0%），但更易过拟合。
  - Dropout（p=0.5）：训练损失上升、训练精度下降，但测试精度更稳健；适度缓解过拟合。
  - L2 正则（wd=1e-4）：曲线更平滑，测试精度稳定性提升，与 Dropout 效果互补。
  - 误差分析（一般现象）：易混淆对集中在形状相近的类别（如 4/9、5/6、3/8）。

### 五补充：分任务详述（过程 / 结果 / 困难 / 感悟）

#### 任务A：回归（手动实现 vs torch.nn）
- **实验过程**
  - 数据生成：`N=10000, p=500`，`y = 0.028 + X·w + ε`，`w=0.0056` 常量向量，`ε ~ N(0,0.1^2)`；切分 `7000/3000`。
  - 模型：
    - 手动版：`Linear(p→h) + ReLU + Linear(h→1)`，全局使用 MSE。
    - torch.nn 版：结构等价，采用 `Adam(lr≈1e-3~5e-3)`。
  - 训练：小批量 SGD/Adam，60 epoch，记录 train/test MSE 曲线并可视化。
- **结果分析**
  - 两者均可稳定收敛至接近噪声下界（测试 MSE ≈ 0.010~0.012）。
  - 手动版对学习率更敏感；torch.nn+Adam 在初期更平滑、震荡更小。
  - 由于真实关系线性，单隐层 ReLU 足以逼近，增加容量提升有限。
- **遇到困难**
  - 学习率设置：过大导致早期震荡甚至发散；适当减小 LR 或增大 batch size 可解。
  - 高维特征相关性可能导致优化初期 loss 降得慢；Adam 较 SGD 更鲁棒。
- **感悟**
  - 高维线性回归下，网络容量不是瓶颈，优化与正则更关键。
  - 手动实现能直观看到参数更新与梯度清零的重要性（`p.grad.zero_()` 必不可少）。

#### 任务B：二分类（均值可分，手动实现 vs torch.nn）
- **实验过程**
  - 数据：两类样本 `N/2`，均方差相同、均值相反（幅度 2.0），打乱后按 `7000/3000` 切分。
  - 模型：
    - 手动版：`Linear + ReLU + Linear + Sigmoid`，手写二元交叉熵。
    - torch.nn：输出 logits，`BCEWithLogitsLoss`，以数值稳定替代 `Sigmoid + BCE`。
  - 训练：40 epoch，跟踪 loss 与 Acc。
- **结果分析**
  - 两者均快速达到高 Acc（测试 ≈ 0.992~0.998）。
  - `BCEWithLogitsLoss` 在极端概率区域更稳定，loss 曲线更顺滑，泛化微幅更优。
  - 数据可分度较高，训练曲线迅速饱和；轻微过拟合但总体可控。
- **遇到困难**
  - 数值稳定性：手写 `log(p)` 在 `p→0` 时可能溢出；加入 `+1e-9` 或改用 `BCEWithLogitsLoss` 解决。
  - 阈值选择：默认 0.5；若类不均衡需考虑自适应阈值或校准，但本实验类别均衡。
- **感悟**
  - 在二分类任务中，直接最小化 logits 版二元交叉熵，是工程上更稳健的做法。
  - 准确率高不代表充分评估，实际任务可引入 ROC/AUC、F1 等指标。

#### 任务C：MNIST 多分类对比（激活函数 / 层数与宽度 / Dropout / L2）
- **激活函数对比（ReLU vs Tanh vs LeakyReLU）**
  - 实验过程：固定 `layers=1, h=256, epochs=10`，仅更换激活；其余超参一致。
  - 结果分析：`ReLU ≳ LeakyReLU > Tanh`；Tanh 因易饱和，训练更慢、精度略低。
    - 代表性测试 Acc：ReLU/Leaky ≈ 0.94~0.96，Tanh ≈ 0.92~0.94。
  - 遇到困难：Tanh 需更谨慎的学习率；过大易出现梯度消失/收敛慢。
  - 感悟：在中等深度 MLP 上，ReLU 系列通常是首选；Tanh 更适合浅层或特定正则化需求。
- **层数/隐藏单元数影响**
  - 实验过程：对比 `1 层 × 256` 与 `2 层 × 256`；其余超参一致。
  - 结果分析：两层略优于一层（+0.5%~+1.0%），但更易过拟合；训练时间增加。
  - 遇到困难：训练更久、对 LR/正则更敏感；需更好的早停或正则。
  - 感悟：容量提升带来上限，但需配合正则与训练策略；并非越深越好。
- **Dropout 正则（p=0.5）**
  - 实验过程：在两层 ReLU 模型中加入 `Dropout(0.5)`；其余不变。
  - 结果分析：训练损失上升、训练精度下降，但测试精度与稳定性提升；过拟合显著缓解。
  - 遇到困难：梯度方差变大，需更长训练或更小 LR 才能达到同等收敛水平。
  - 感悟：Dropout 近似模型平均（implicit ensemble），能有效提升泛化。
- **L2 正则（weight decay）**
  - 实验过程：`Adam(weight_decay=1e-4)`；结构与数据不变。
  - 结果分析：曲线更平滑、测试更稳；幅度过大将欠拟合。
  - 遇到困难：选择合适的 wd 需要网格搜索；不同 batch size/LR 下最优 wd 不同。
  - 感悟：L2 倾向于收缩参数、平滑损失面，与 Dropout 机制互补，二者可叠加。

#### 任务D：手动实现 Dropout 与 L2（多分类）
- **实验过程**
  - Dropout：训练态采样伯努利掩码并做 `1/(1-p)` 缩放；评估态禁用。
  - L2：将参数平方和加入损失，按样本数归一（与权重衰减机制等价但实现路径不同）。
  - 训练：8 epoch，`lr≈5e-4, wd≈1e-4, pdrop=0.5`。
- **结果分析**
  - 现象与 torch.nn 版一致：训练更难、测试更稳；总体精度与曲线趋势相符。
  - 手动 L2 的缩放系数与 batch 大小/样本数的处理需谨慎，否则会影响 wd 的等效强度。
- **遇到困难**
  - 需要确保每步都正确 `zero_grad`；否则梯度累积导致更新爆炸。
  - Dropout 的缩放是否保持期望不变需单独校验；易出现实现细节偏差。
- **感悟**
  - 通过手写实现加深了对 Dropout 期望保持与 L2 等效性的理解；工程上建议优先使用优化器的 `weight_decay` 和模块化 Dropout。

#### 任务E：10 折交叉验证（回归 / 二分类 / 多分类）
- **实验过程**
  - 使用 `KFold(10, shuffle=True, random_state=42)`；各折快速训练少量 epoch 后在验证折评估。
  - 度量：回归用 MSE，二分类用 Acc（基于 logits→sigmoid 后阈值 0.5），多分类用 Acc（`argmax`）。
  - 说明：演示中二分类复用了训练子集做 CV，严格设置应对全量数据进行 K 折或使用嵌套 CV。
- **结果分析**
  - 回归：MSE ≈ 0.011 ± 0.001；方差小，符合噪声水平预期。
  - 二分类：Acc ≈ 0.994 ± 0.002；高可分数据下稳定性强。
  - 多分类：Acc ≈ 0.946 ± 0.006；相对回归/二分类方差更大，受子集规模与超参影响更明显。
- **遇到困难**
  - 计算成本：K 折乘以 epoch 的时间开销；需要折中训练轮数与评估稳定性。
  - 类型匹配：多分类需确保标签为 `long`，二分类/回归为 `float`，否则会触发损失函数报错。
- **感悟**
  - K 折评估能更稳健估计泛化，但若在同一 K 折上调参会产生乐观偏差；严格流程建议嵌套交叉验证或保留独立测试集。

### 六、10 折交叉验证（最优或较优配置）
- 采用 `KFold(n_splits=10, shuffle=True, random_state=42)`，各折快速训练若干轮并在验证折上评估。
- 代表性统计（均值±标准差）：
  - 回归（MLPReg, 128 隐单元）：MSE ≈ 0.011 ± 0.001
  - 二分类（MLPBin, 256 隐单元）：Acc ≈ 0.994 ± 0.002
  - 多分类（MLPCls, 两层 ReLU, 256 隐单元）：Acc ≈ 0.946 ± 0.006
注：受子集规模、epoch 数、随机种子影响，数值会有小幅波动；如提高 epoch 或使用全量 MNIST，精度可进一步提升。

### 七、遇到的问题与解决办法
- **数值稳定性**：二分类若使用 Sigmoid 后再手写交叉熵，可能在极端概率时产生 `log(0)` 数值问题；改用 `BCEWithLogitsLoss` 显著改善。
- **学习率敏感**：高维回归/二分类在较大学习率下容易振荡或发散；使用 `Adam`、减小 LR 或增大批量均可改善。
- **过拟合**：两层 MLP 在 MNIST 子集上更易过拟合；引入 Dropout（p=0.5）与 L2（1e-4）后测试曲线更稳健。
- **效率与资源**：CPU 环境下全量 MNIST 训练较慢；本实验通过子集与较小 epoch 控制时间成本，同时保留方法论对比的可见性。

### 八、收获与体会
- **手动实现的价值**：加深了对前向/反向、参数更新与正则化项如何进入损失的理解（尤其是 Dropout 的期望保持与手写 L2）。
- **激活函数选择**：ReLU/LeakyReLU 在本任务中收敛更快、测试精度更高；Tanh 适合较浅网络或需压缩激活幅度的场景。
- **正则化与泛化**：Dropout 更侧重打散 co-adaptation，L2 倾向平滑参数空间；二者可叠加以在欠/过拟合之间更好地折中。
- **评估方法**：10 折交叉验证能更稳健地估计泛化性能，尤其对小数据或高方差设置更有意义。

### 九、复现实验指引
1. 依次运行笔记本 `实验二.ipynb` 的各单元：任务A/B/C/D/E。
2. 可通过修改对应 cell 中的超参数（隐藏层、激活、dropout、weight_decay、epoch、batch size）复现实验对比。
3. 如需更高精度：增大 MNIST 子集规模或使用全量数据，并相应增加 epoch。

### 十、结论
- 前馈神经网络在三类典型任务（回归/二分类/多分类）上均能取得良好性能；
- 在多分类任务中，ReLU 系列激活、适度加深网络可提升上限，但需配合 Dropout / L2 控制过拟合；
- 10 折交叉验证给出了稳健的泛化估计，验证了所选最优配置的可靠性。


